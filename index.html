<html>
<head>
  <style>
    body {
      background-color: lightcoral;
      overflow-x: hidden;
    }
    .bigBubble {
      background-color: lightpink;
      position:absolute;
      top:-125vh;
      left:-50vh;
      width: 200vh;
      height: 200vh;
      border-radius:100vh;
    }
    .mediumBubble {
      background-color: lightgrey;
      position:absolute;
      top:-125vh;
      left:-18.75vh;
      width: 150vh;
      height: 150vh;
      border-radius:75vh;
    }
    .smallBubble {
      background-color: lightblue;
      position:absolute;
      top:-50vh;
      left:20vh;
      width: 60vh;
      height: 60vh;
      border-radius:30vh;
    }
  </style>
</head>
<body>
  <div class="bigBubble"></div>
  <div class="mediumBubble"></div>
  <div class="smallBubble"></div>
  <script>
    const audioCtx = new AudioContext();
    var originAudioBuffer;
    let duration = 0; //messy
    const maxDecibels = -30;
    const minDecibels = -50;
    async function load() {     
      const response = await fetch('sample.mp3');
      const arrayBuffer = await response.arrayBuffer();
      originAudioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
      duration = originAudioBuffer.duration; //messy
      const trackSource = audioCtx.createBufferSource();
      trackSource.buffer = originAudioBuffer;
      trackSource.connect(audioCtx.destination);
      return trackSource;
    }
    async function handleMicrophoneSuccess(stream) {
      let trackSource = await load();
      trackSource.addEventListener('ended', () => {
        mediaRecorder.stop();
      });
      const trackAnal = audioCtx.createAnalyser();
      trackAnal.maxDecibels = maxDecibels;
      trackAnal.minDecibels = minDecibels;
      trackAnal.smoothingTimeConstant = 0.9;
      trackAnal.fftSize = 32;
      const trackProcessor = audioCtx.createScriptProcessor(1024, 1, 1);
      let trackAmplitudeArray = new Uint8Array(trackAnal.frequencyBinCount);
      trackSource.connect(trackAnal);
      trackAnal.connect(trackProcessor);
      let trackAvg = 0;
      let trackTotalOfSamples = 0;
      let trackSamples = 0;
      trackProcessor.onaudioprocess = function() {
        //console.log('processing'); //works
        //anal.getByteTimeDomainData(amplitudeArray);
        trackAnal.getByteFrequencyData(trackAmplitudeArray);
        //console.log(amplitudeArray);
        let sampleTotal = (trackAmplitudeArray[0]+trackAmplitudeArray[1])/2;
        if(sampleTotal > 0) {
          trackTotalOfSamples += sampleTotal;
          trackSamples++;
          trackAvg = trackTotalOfSamples/trackSamples;
        }
        //console.log('trackAvg' + trackAvg);
        //console.log(trackAmplitudeArray);
      }

      const streamSource = audioCtx.createMediaStreamSource(stream);
      const anal = audioCtx.createAnalyser();
      anal.maxDecibels = maxDecibels;
      anal.minDecibels = minDecibels;
      anal.smoothingTimeConstant = 0.9;
      anal.fftSize = 32;
      const processor = audioCtx.createScriptProcessor(1024, 1, 1);
      let amplitudeArray = new Uint8Array(anal.frequencyBinCount);
      streamSource.connect(anal);
      anal.connect(processor);
      let avg = 0;
      let totalOfSamples = 0;
      let samples = 0;
      processor.onaudioprocess = function() {
        console.log('processing');
        anal.getByteFrequencyData(amplitudeArray);
        let sampleTotal = (amplitudeArray[0]+amplitudeArray[1])/2;
        //console.log(sampleTotal);
        if(sampleTotal > 0) {
          totalOfSamples += sampleTotal;
          samples++;
          avg = totalOfSamples/samples;
        }
        //console.log(avg);
        //console.log(amplitudeArray)
      }

      const options = {mimeType: 'audio/webm'};
      const recordedChunks = [];
      const mediaRecorder = new MediaRecorder(stream, options);

      mediaRecorder.addEventListener('dataavailable', async function(e) { //assuming this event only happens after recording 
        trackSource.disconnect();
        trackAnal.disconnect();
        streamSource.disconnect();
        anal.disconnect();
        if (e.data.size > 0) {
          recordedChunks.push(e.data);
        }
        const recordedChunksBlob = new Blob(recordedChunks);
        const offlineAudioCtx = new OfflineAudioContext(1, duration*48000, 48000);
        const originTrackSource = offlineAudioCtx.createBufferSource();
        originTrackSource.buffer = trackSource.buffer;
        originTrackSource.connect(offlineAudioCtx.destination);

        const recordedArrayBuffer = await recordedChunksBlob.arrayBuffer();
        const recordedAudioBuffer = await audioCtx.decodeAudioData(recordedArrayBuffer);
        const recordedTrackSource = offlineAudioCtx.createBufferSource();
        recordedTrackSource.buffer = recordedAudioBuffer;
        const gain = offlineAudioCtx.createGain();
        gain.gain.value = (trackAvg - avg)/255*(maxDecibels- minDecibels);
        recordedTrackSource.connect(gain);
        gain.connect(offlineAudioCtx.destination);

        originTrackSource.start();
        recordedTrackSource.start();
        const render = await offlineAudioCtx.startRendering();
        const mixRawData = render.getChannelData(0);
        const mixDataBlob = new Blob(mixRawData);
        var mix = audioCtx.createBufferSource();
        mix.buffer = render;
        mix.connect(audioCtx.destination);
        mix.start();
      });

      mediaRecorder.start();
      trackSource.start();

    };

    //navigator.mediaDevices.getUserMedia({ audio: true, video: false }).then(handleMicrophoneSuccess);
  </script>
</body>
</html>